name: Run LangSmith Eval

on:
  workflow_dispatch:
    inputs:
      dataset:
        description: "LangSmith dataset name or ID"
        required: true
        type: string
      experiment_prefix:
        description: "Experiment prefix for this run"
        required: true
        type: string
        default: manual

jobs:
  eval:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install the LangGraph agent package in editable mode
          pip install -e ai/langchain
          # Install eval-time dependencies used by evaluators and runner
          pip install "langchain>=0.2" "langchain-core>=0.2" "langchain-huggingface>=0.1" langsmith openevals python-dotenv langchain-openai aiohttp

      - name: Run evaluator
        env:
          # Required by evaluators (OpenAI judge) and graph (LangSmith + Hugging Face)
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_TRACING: true
          HUGGINGFACEHUB_API_TOKEN: ${{ secrets.HUGGINGFACEHUB_API_TOKEN }}
        run: |
          python -c "import sys; sys.path.insert(0, 'ai/langchain/tests/integration_tests'); from eval_runner import main; import sys as _s; _s.exit(main(['eval', '${{ inputs.dataset }}', '${{ inputs.experiment_prefix }}']))"


